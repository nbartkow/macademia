== Stage 0: DocIDGenerator ==
It generates ID and names for the chosen Wikipedia pages.
=== Optimizations ===
   * Excludes any disambiguation, empty, redirect and stub pages
   * Excludes any words in the set of stop words
=== Initial Input ===
   * Wikipedia pages
=== Final output ===
   * A page name and ID (drama in the 1970's "\t" 12341367) a copy of this output is in MongoDB in the Poliwiki server
=== Mapper ===
   * Key - this is not the Wikipedia page ID, it is only a long value showing where in the file the page is
   * Value - this is the actual Wikipedia page: contains the actual text, page ID and other information.
=== Reducer ===
   * Key - page Title
   * Value - page ID
=== Hadoop job logistics ===
   * Can use the medium Amazon elastic cluster to run it ( 20 mappers and 20 reducers)
   * The input file is "s3://macademia/wiki/input" and the output file is "s3://macademia/step0"


== Stage 1: Wordcounter ==
It takes each Wikipedia page and counts how many times a word appears in the page.
=== Optimizations ===
   * Only add pages that are in the ranked_pages file. The file is produces by a python file that is produced by combining the wikipedia_cache and pagecounts files. This allows to only add pages past a certain rank, no hard rank threshold needed.
=== Initial Input ===
   * wikipedia pages and rankedpages.txt
=== Final Ouput ===
   * A word, the wikipedia page ID, the number of times the word appears in the page and total number of words processed in that page (developed   @345263746@5@6249).
   * A word and the total count of Wikipedia pages from which the word was processed (developed#c1234). 
=== Mapper ===
   * Key - this is not the Wikipedia page ID, it is only a long value showing where in the file the page is 
   * Value: Wikipedia page

=== Reducer ===
   * Key - word
   * Value: a Wikipedia page, number of times the word appeared in the page, the number of words in the page.             


== Stage 2:Docfrequency ==
Prepares the data pieces for the tf-idf calculations

=== Initial Input ===
   * output from step 1.
=== Final Ouput ===
   * Each line contains a Wikipedia page, a word, the number of times the word appears in the article, the length of the page and the number of times the  
=== Mapper ===
   * Identity

=== Reducer ===
   * Input: word #cnumber_of_occurrences_across_all_wikipages
            word @ page_title @ word_count @ total_number_words 
            
   * Output: wiki_page word@articleWordFrequency@articleWordCount@wordDocFrequency
   * Output Example: Anarchism   developed@345@6249@32424 

== Stage 3:TfidfVectorPruner ==
This is used to calculate the tf-idf scores

=== Mapper ===
   * Identity

=== Reducer ===
   * Input: wiki_page_name word@articleWordFrequency@articleWordCount@wordDocFrequency
   * Ouput: article word@tf-idf_score
   * Output Example: "Anarchism   developed@32.0"


== Stage 4: DocSimScorer ==
Calculates the similarities between two documents

=== Mapper ===
   * Input: wiki_page_name word@tf-idf_score
   * Output: word wiki_page_name@score
   * Output Example: developed Anarchism@32

=== Reducer ===
   * Input: word wiki_page_name@score
   * Output: wiki_page_name1@wiki_page_name2 (wp1.score*wp2.score) -- the multiplication of the scores is for cosine similarity
   * Output example: Anarchism Decency@0.43

== Stage 5 ==
This adds the sim_scores for the pair of wiki articles being compared

=== Mapper ===
   * Identity

=== Reducer ===
   * input: wiki_page_name1 wiki_page_name2@(wp1.score*wp2.score)
   * Output: wiki_page_name1@wiki_page_name2 Combined_similairty_scores
   * Anarchism@Decency 0.48