== Stage 0: DocIDGenerator ==
It generates ID and names for the chosen Wikipedia pages.

=== Optimizations ===
   * Excludes any disambiguation, empty, redirect and stub pages
   * Excludes any words in the set of stop words
=== Initial Input ===
   * Wikipedia pages
=== Final output ===
   * A page name and ID (drama in the 1970's "\t" 12341367)
=== Mapper ===
   * Key - this is not the Wikipedia page ID, it is only a long value showing where in the file the page is
   * Value - tis is the actual Wikipedia page: contains the actual text, page ID and other information, look up "WikipediaPage" to see what it contains.
=== Reducer ===
   * Key - page Title
   * Value - page ID
=== Hadoop job logistics ===
   * Can use the medium Amazon elastic cluster to run it ( 20 mappers and 20 reducers)
   * The input file is "s3://macademia/wiki/input" and the output file is "s3://macademia/step0"



== Stage 1: WordCounter ==
It takes each Wikipedia page and counts how many times a word appears in the page.

=== Mapper ===
   * Input: wikipedia pages
   * Output: word @ page_title @ word_count @ total_number_words
   * Example: Mathematics @Modern_Mathematics@12@120 per line

=== Reducer ===
   * Input: word @ page_title @ word_count @ total_number_words
   * Outputs: word #cnumber_of_occurrences_across_all_wikipages
              word @ page_title @ word_count @ total_number_words              

   * Output Example: Mathematics #c1230
                     Mathematics @Modern_Mathematics@12@120          



== Stage 2:DocFrequency ==
Prepares the data pieces for the tf-idf calculations

=== Mapper ===
   * Identity

=== Reducer ===
   * Input: word #cnumber_of_occurrences_across_all_wikipages
            word @ page_title @ word_count @ total_number_words 
            
   * Output: wiki_page word@articleWordFrequency@articleWordCount@wordDocFrequency
   * Output Example: Anarchism   developed@345@6249@32424 

== Stage 3:TfidfVectorPruner ==
This is used to calculate the tf-idf scores

=== Mapper ===
   * Identity

=== Reducer ===
   * Input: wiki_page_name word@articleWordFrequency@articleWordCount@wordDocFrequency
   * Ouput: article word@tf-idf_score
   * Output Example: "Anarchism   developed@32.0"

== Stage 4: DocSimScorer ==
Calculates the similarities between two documents

=== Mapper ===
   * Input: wiki_page_name word@tf-idf_score
   * Output: word wiki_page_name@score
   * Output Example: developed Anarchism@32

=== Reducer ===
   * Input: word wiki_page_name@score
   * Output: wiki_page_name1@wiki_page_name2 (wp1.score*wp2.score) -- the multiplication of the scores is for cosine similarity
   * Output example: Anarchism Decency@0.43

== Stage 5 ==
This adds the sim_scores for the pair of wiki articles being compared

=== Mapper ===
   * Identity

=== Reducer ===
   * input: wiki_page_name1 wiki_page_name2@(wp1.score*wp2.score)
   * Output: wiki_page_name1@wiki_page_name2 Combined_similairty_scores
   * Anarchism@Decency 0.48